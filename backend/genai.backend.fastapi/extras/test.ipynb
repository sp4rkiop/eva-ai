{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from os import getenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n",
    "endpoint = getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# endpoint = getenv(\"AZURE_OPENAI_ENDPOINT\").format(getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),  getenv(\"AZURE_OPENAI_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {word} in 100 words.\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful translator. Translate the user sentence to French.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0.01,\n",
    "    stream_usage=True\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [  \n",
    "(  \n",
    "\"system\",  \n",
    "\"You are a helpful translator. Translate the user sentence to French.\",  \n",
    "),  \n",
    "(\"human\", \"I love programming.\"),  \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 28, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BaUDFPAFs2smbF11kYf2UkyY9cNe0', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--53debc16-64da-43be-b7b2-5b31512131b2-0', usage_metadata={'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.ainvoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='histoire sur Terre' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 27, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BaV2XvCohIQJaJ53D0Calgk8GJmiu', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--ee9f9529-a6ac-4a9a-9b63-fe01dbe8717b-0' usage_metadata={'input_tokens': 27, 'output_tokens': 5, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# get the full response at once\n",
    "if chain:\n",
    "    response = await chain.ainvoke({\"input\": \"story on earth\"})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'adore la programmation.\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0'} id='run--0c4d065d-a4d6-4abb-82de-e751b4a7be11' usage_metadata={'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# stream the response\n",
    "full_message = None\n",
    "\n",
    "async for chunk in chain.astream({\"input\": \"on earth\"}):\n",
    "    # print(chunk.content, end=\"\", flush=True)\n",
    "    if full_message is None:\n",
    "        full_message = chunk\n",
    "    else:\n",
    "        full_message += chunk  # Uses the overloaded __add__ method\n",
    "\n",
    "# After streaming is complete, full_message contains the aggregated content and metadata\n",
    "print(full_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessageChunk'>\n"
     ]
    }
   ],
   "source": [
    "print(type(full_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat history in memory with proxy history upto last 2 messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "class ProxyHistory(BaseChatMessageHistory):\n",
    "    def __init__(self, full_history: InMemoryHistory, limit: int = 2):\n",
    "        self._full_history = full_history\n",
    "        self._limit = limit\n",
    "\n",
    "    @property\n",
    "    def messages(self) -> list[BaseMessage]:\n",
    "        # Only return the last N messages\n",
    "        return self._full_history.messages[-self._limit:]\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        # Delegate writes to the original history\n",
    "        self._full_history.add_messages(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self._full_history.clear()\n",
    "\n",
    "\n",
    "store = {}\n",
    "session_id = \"student1234\"\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return ProxyHistory(store[session_id], limit=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {word} in 100 words.\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "parser = StrOutputParser()\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0.01,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in chain_with_history.astream({\"input\": \"how many messages we have in this chat? can you list them with roles?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "serialized_store = pickle.dumps(store)\n",
    "print(serialized_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_store = pickle.loads(serialized_store)\n",
    "store=re_store\n",
    "print(re_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat history with Scylla DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keyspace and tables\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "# Connect to ScyllaDB\n",
    "cluster = Cluster(['127.0.0.1'])  # Replace with your Scylla host IP if different\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace (if not exists)\n",
    "KEYSPACE = \"eva\"\n",
    "session.execute(f\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "    WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': '1'}}\n",
    "\"\"\")\n",
    "\n",
    "# Set keyspace\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# Create tables\n",
    "table_queries = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS chathistory (\n",
    "        userid uuid,\n",
    "        chatid uuid,\n",
    "        visible boolean,\n",
    "        chathistoryjson blob,\n",
    "        chattitle text,\n",
    "        createdon timestamp,\n",
    "        nettokenconsumption int,\n",
    "        PRIMARY KEY (userid, chatid)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS chathistory_by_visible AS\n",
    "    SELECT userid, chatid, chathistoryjson, chattitle, createdon, nettokenconsumption\n",
    "    FROM chathistory\n",
    "    WHERE visible IS NOT NULL AND userid IS NOT NULL AND chatid IS NOT NULL\n",
    "    PRIMARY KEY (visible, userid, chatid);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS availablemodels (\n",
    "        deploymentid uuid,\n",
    "        isactive boolean,\n",
    "        apikey text,\n",
    "        deploymentname text,\n",
    "        endpoint text,\n",
    "        modelname text,\n",
    "        modeltype text,\n",
    "        modelversion text,\n",
    "        provider text,\n",
    "        PRIMARY KEY ((deploymentid), isactive)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS usersubscriptions (\n",
    "        userid uuid,\n",
    "        modelid uuid,\n",
    "        PRIMARY KEY (userid, modelid)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        email text,\n",
    "        partner text,\n",
    "        userid uuid,\n",
    "        firstname text,\n",
    "        lastname text,\n",
    "        role text,\n",
    "        PRIMARY KEY ((email, partner), userid)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for query in table_queries:\n",
    "    session.execute(query)\n",
    "\n",
    "print(\"Keyspace and tables created successfully.\")\n",
    "rows = session.execute(f\"SELECT table_name FROM system_schema.tables WHERE keyspace_name='{KEYSPACE}';\")\n",
    "for row in rows:\n",
    "    print(\"Table:\", row.table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage, FunctionMessage, trim_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: Sequence[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "    \n",
    "    def edit_message_at_index(self, index: int, new_message: BaseMessage) -> None:\n",
    "        if 0 <= index < len(self.messages):\n",
    "            self.messages[index] = new_message\n",
    "            # Truncate all messages after the edited one\n",
    "            self.messages = self.messages[:index + 1]\n",
    "        else:\n",
    "            raise IndexError(\"Message index out of range.\")\n",
    "\n",
    "store = {}\n",
    "branch = \"main\"\n",
    "\n",
    "# def get_chat_history_by_branch(branch: str) -> BaseChatMessageHistory:\n",
    "#     if branch not in store:\n",
    "#         store[branch] = InMemoryHistory()\n",
    "#     return store[branch]\n",
    "def get_chat_history_by_branch(branch: str) -> BaseChatMessageHistory:\n",
    "    if branch not in store:\n",
    "        store[branch] = InMemoryHistory()\n",
    "    return store[branch]\n",
    "\n",
    "def append_message_to_branch(message: BaseMessage, branch: str) -> None:\n",
    "    if branch not in store:\n",
    "        store[branch] = InMemoryHistory()\n",
    "    store[branch].messages.append(message)\n",
    "\n",
    "def create_branch_from(parent_branch: str, new_branch: str, edit_index: int, new_message: BaseMessage):\n",
    "    parent_history = store.get(parent_branch)\n",
    "    if not parent_history:\n",
    "        raise ValueError(f\"Parent branch '{parent_branch}' does not exist.\")\n",
    "\n",
    "    # Clone messages up to the edit point\n",
    "    new_history = InMemoryHistory(messages=deepcopy(parent_history.messages[:edit_index]))\n",
    "    # Add the new edited message\n",
    "    new_history.add_messages([new_message])\n",
    "    # Store the new branch\n",
    "    store[new_branch] = new_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = StrOutputParser()\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0.01,\n",
    "    stream_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_input(input):\n",
    "    chat_history = get_chat_history_by_branch(branch).messages[-4:]\n",
    "    print(chat_history)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a strict teacher who gives concise answers in 1 word\"),\n",
    "        *chat_history,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_session_history=get_chat_history_by_branch,\n",
    "        input_messages_key=\"input\",\n",
    "        history_factory_config=[\n",
    "            ConfigurableFieldSpec(\n",
    "                id=\"branch\",\n",
    "                annotation=str,\n",
    "                name=\"Chat Branch Name\",\n",
    "                description=\"Unique name for the chat branch\",\n",
    "                default=\"\",\n",
    "                is_shared=True,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    # ai_message =\"\"\n",
    "    async for chunk in chain_with_history.astream({\"input\": input},\n",
    "                                                  config={\"configurable\": {\"branch\": branch}}):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "        # ai_message += chunk\n",
    "    # InMemoryHistory().add_messages([HumanMessage(content=input), AIMessage(content=ai_message)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch=\"part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='what is earth', additional_kwargs={}, response_metadata={}), AIMessage(content='Planet.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is earth', additional_kwargs={}, response_metadata={}), AIMessageChunk(content='Planet.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0'}, id='run--93093a8f-bfa9-478c-aa94-c5d009c5205a')]\n",
      "content='' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='Clar' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='ify' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='.' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0'} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573'content='' additional_kwargs={} response_metadata={} id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573' usage_metadata={'input_tokens': 215, 'output_tokens': 4, 'total_tokens': 219, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}"
     ]
    }
   ],
   "source": [
    "await process_input(HumanMessage(content=\"op\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main': InMemoryHistory(messages=[HumanMessage(content='are you teacher?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is earth', additional_kwargs={}, response_metadata={}), AIMessage(content='Planet.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is earth', additional_kwargs={}, response_metadata={}), AIMessageChunk(content='Planet.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0'}, id='run--93093a8f-bfa9-478c-aa94-c5d009c5205a'), HumanMessage(content='op', additional_kwargs={}, response_metadata={}), AIMessageChunk(content='Clarify.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0'}, id='run--0f30f0a9-380e-4016-85aa-7cfeb1f14573', usage_metadata={'input_tokens': 215, 'output_tokens': 4, 'total_tokens': 219, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Create a new branch 'feature' by editing message at index 2 ('c') to 'g'\n",
    "create_branch_from(\n",
    "    parent_branch='main',\n",
    "    new_branch='b',\n",
    "    edit_index=0,\n",
    "    new_message=HumanMessage(content=\"check\")\n",
    ")\n",
    "\n",
    "# Add a new message 'j' to the 'feature' branch\n",
    "store['b'].add_messages([AIMessage(content=\"j\")])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
